# LLM Inference Optimization

## Quantization Algorithms
Currently I'm working with [llama.cpp](https://github.com/ggml-org/llama.cpp) and understading it's working on int8 and int4 quantization. I will be coding this and publishing the code and documentation on [100-day CUDA Challenge under Day 13](https://github.com/fasih0001/100-days-of-GPU). Actually, my whole motivation in the end is to understand and reproduce the algorithm mentioned in paper [QuantSpec: Self-Speculative Decoding with Hierarchical Quantized KV Cache](https://arxiv.org/abs/2502.10424)


