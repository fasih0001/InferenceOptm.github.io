# LLM Inference Optimization

## Qunatization Algorithms
Currently I'm working with llama.cpp and understading it's working on int8 and int4 quantization. I will be coding this and publishing the code on [100-day CUDA Challenge under Day 13](https://github.com/fasih0001/100-days-of-GPU). Actually, my whole motivation in the end is to understand and reproduce the algorithm mentioned in paper ([QuantSpec: Self-Speculative Decoding with Hierarchical Quantized KV Cache](https://arxiv.org/abs/2502.10424))


