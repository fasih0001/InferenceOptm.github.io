{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"LLM Inference Optimization Quantization Algorithms Currently I'm working with llama.cpp and understading it's working on int8 and int4 quantization. I will be coding this and publishing the code and documentation on 100-day CUDA Challenge under Day 13 . Actually, my whole motivation in the end is to understand and reproduce the algorithm mentioned in paper QuantSpec: Self-Speculative Decoding with Hierarchical Quantized KV Cache","title":"Home"},{"location":"#llm-inference-optimization","text":"","title":"LLM Inference Optimization"},{"location":"#quantization-algorithms","text":"Currently I'm working with llama.cpp and understading it's working on int8 and int4 quantization. I will be coding this and publishing the code and documentation on 100-day CUDA Challenge under Day 13 . Actually, my whole motivation in the end is to understand and reproduce the algorithm mentioned in paper QuantSpec: Self-Speculative Decoding with Hierarchical Quantized KV Cache","title":"Quantization Algorithms"},{"location":"attention-kernels/fp16-attention/","text":"","title":"Baseline FP16 Attention"},{"location":"attention-kernels/int4-attention/","text":"","title":"Quantized Attention (INT4)"},{"location":"attention-kernels/int8-attention/","text":"","title":"Quantized Attention (INT8)"},{"location":"attention-kernels/warp-level/","text":"","title":"Warp-Level Optimization"},{"location":"background/ggml-cuda-mapping/","text":"I will study this first.","title":"Where QuantSpec Lives in ggml-cuda"},{"location":"background/ggml-cuda-mapping/#i-will-study-this-first","text":"","title":"I will study this first."},{"location":"background/quantspec-overview/","text":"","title":"QuantSpec Overview"},{"location":"foundations/memory-bound-decoding/","text":"","title":"Memory-Bound Decoding"},{"location":"foundations/roofline/","text":"","title":"Roofline & Arithmetic Intensity"},{"location":"hierarchical-kv/hierarchical-kv/","text":"","title":"Hierarchical INT4 KV Cache"},{"location":"hierarchical-kv/kv-bandwidth/","text":"","title":"Bandwidth Analysis"},{"location":"hierarchical-kv/kv-cache-anatomy/","text":"","title":"KV Cache Anatomy"},{"location":"modeling-quantspec/amortization/","text":"","title":"Amortized Quantization Cost"},{"location":"modeling-quantspec/double-buffer/","text":"","title":"Double FP Buffer Concept"},{"location":"performance-reproduction/kernel-benchmarks/","text":"","title":"Kernel Benchmarks"},{"location":"performance-reproduction/speedup-analysis/","text":"","title":"Speedup Analysis"},{"location":"performance-reproduction/table5-mapping/","text":"","title":"Mapping QuantSpec Table 5"},{"location":"quantization-basics/int4-packing/","text":"","title":"INT4 Packing"},{"location":"quantization-basics/int4-reconstruction/","text":"","title":"INT4 Residuals & Reconstruction"},{"location":"quantization-basics/int8-packing/","text":"","title":"INT8 Packing"},{"location":"reductions-softmax/block-reductions/","text":"","title":"Block Reductions"},{"location":"reductions-softmax/numerical-stability/","text":"","title":"Numerical Stability"},{"location":"reductions-softmax/warp-reductions/","text":"","title":"Warp Reductions"},{"location":"synthesis/synthesis/","text":"","title":"Reproducing QuantSpec from First Principles"}]}