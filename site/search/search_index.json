{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"LLM Inference Optimization Qunatization Algorithms Currently I'm working with llama.cpp and understading it's working on int8 and int4 quantization. I will be coding this and publishing the code on 100-day CUDA Challenge under Day 13 . Actually, my whole motivation in the end is to understand and reproduce the algorithm mentioned in paper ( QuantSpec: Self-Speculative Decoding with Hierarchical Quantized KV Cache )","title":"Home"},{"location":"#llm-inference-optimization","text":"","title":"LLM Inference Optimization"},{"location":"#qunatization-algorithms","text":"Currently I'm working with llama.cpp and understading it's working on int8 and int4 quantization. I will be coding this and publishing the code on 100-day CUDA Challenge under Day 13 . Actually, my whole motivation in the end is to understand and reproduce the algorithm mentioned in paper ( QuantSpec: Self-Speculative Decoding with Hierarchical Quantized KV Cache )","title":"Qunatization Algorithms"}]}